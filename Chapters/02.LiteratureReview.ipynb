{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Literature Review\n",
    "\n",
    "[index](../Index.ipynb) | [prev](./01.Introduction.ipynb) | [next](./03.SystemDesign.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to appreciate and highlight the work of other researchers, without which my concept would never be possible and a project like this could not be delivered by a single person.\n",
    "\n",
    "This Literature Review will follow the data flow in the process. There are a three main building blocks in this research:\n",
    "- Data Collection\n",
    "- Forecasting\n",
    "- Anomaly Detection\n",
    "\n",
    "Each of them is very complex and has been studied by scientists for decades.\n",
    "\n",
    "After two years of my own research and testing, I have narrowed down the list to a handful of useful tools, for which I will provide the theoretical background below.\n",
    "\n",
    "Before any discussions about Machine Learning models, I would like to point out a fundamental concept, which will be the guide through the rest of my work in this research: **Bias and Variance tradeoff**.\n",
    "\n",
    "We say that a model is good if it fits the training and testing data well. Models like Linear Regression create a straight line through the data points, and often do not represent the relationships very well. This is called **High Bias**. On the other hand, Learners like Decision Trees model relationships in the training data very well, but tend to perform poorly on the testing sets. We call this behaviour **High Variance**. Ideally we always look for a model with relatively low bias and low variance. In practice, it is a matter of finding a good tradeoff.\n",
    "\n",
    "Note that the above tradeoff tends to hold for non-Neural Network models, but not necessarily for Neural Networks, which can generalise well, even with their complexity and High Variance (Neal et al., 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Data Collection\n",
    "\n",
    "The first aspect of my project is the Data Collection phase.\n",
    "\n",
    "It involves a mini-computer (Raspberry Pi) which streams the data to the central unit (a Ubuntu-based Desktop PC with a GPU), which runs an infinite loop with the two key algorithms:\n",
    "- Backgroud subtraction\n",
    "- Yolo Object Recognition\n",
    "\n",
    "Both of these algorithms are extremelly useful in the image processing applications, and the are the foundation of how data is collected in the system.\n",
    "\n",
    "They have both significantly reduced the data size, which otherwise would be required to store a six months of video footage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Motion Detection\n",
    "\n",
    "Let’s consider how can we detect objects of our interest in the video streams.\n",
    "\n",
    "The naive approach would be to send all 30 frames per second to an object detector script.\n",
    "\n",
    "This could work, but it would be extremely inefficient from the compute resource utilisation (CPU/GPU/RAM) and speed perspective. Why would we waste the resources to detect objects if there is actually nothing changing in the scene most of the time? Can we somehow detect a significant change in the images (i.e. detect motion) and only if motion is detected use the object detector?\n",
    "\n",
    "Here is an example of a static background:\n",
    "\n",
    "<p style=\"text-align: center; margin-bottom: 0;\">Fig. 2.1</p>\n",
    "<img src=\"../Resources/img/no-motion.png\" style=\"width: 40%;\"/><br>\n",
    "\n",
    "And here is a moving object (a person running) in a 7 consecutive frames:\n",
    "\n",
    "<p style=\"text-align: center; margin-bottom: 0;\">Fig. 2.2</p>\n",
    "<img src=\"../Resources/img/moving-object.png\" style=\"width: 90%;\"/>\n",
    "\n",
    "It turns out that there is already a set well established algorithms in the Computer Vision domain for that purpose. They are not 100% bulletproof, but they do not need to be. If they can help to reject the 90% of non-interesting frames, we can dedicate the expensive GPU resource to other tasks or simply extend its lifespan. The bonus of not using GPU all the time is that the machine will be much quieter and won’t generate so much heat.\n",
    "\n",
    "One of the most popular and successful methods for motion detection in images is the **Background Subtraction**.\n",
    "\n",
    "At a very high level the concept is very simple: we would like to start with a static image without any moving objects. We will call it the **background ($BG$)**. Then, every consecutive frame will be compared against the background to detect any changes in the **foreground ($FG$)**.\n",
    "\n",
    "Unfortunately, there are many challenges in this optimistic approach. What if:\n",
    "- the initial background already contains moving objects?\n",
    "- the next frames actually don’t contain any moving objects, but only light illumination has changed over time?\n",
    "- shadows started to appear?\n",
    "- the camera is in-door and we turn the light on and off?\n",
    "- there are moving tree branches in the background?\n",
    "- the weather has changed into rain or snow?\n",
    "- we are not interested in small objects, but only objects of a certain size?\n",
    "\n",
    "These common anomalies demand a more sophisticated approach than just a simple subtraction of foreground from the background.\n",
    "\n",
    "One very popular improvement over the vanilla algorithm has been proposed in the *Improved Adaptive Gaussian Mixture Model for Background Subtraction* paper (Zivkivoc 2004). The aim of Zivkivoc’s work was to overcome all the challenges above and achieve efficiency by reducing the processing time.\n",
    "\n",
    "In the MOG2 model, the background is constantly updated and not static. As author describes it, it uses recursive equations to constantly update parameters and also select appropriate number of components per each pixel. At a high level author describes a metric R (using a Bayesian decision), which follows the formula:\n",
    "\n",
    "$$\n",
    "R=\\frac{p(BG|\\overrightarrow{x}^{(t)})}{p(FG|\\overrightarrow{x}^{(t)})}=\\frac{p(\\overrightarrow{x}^{(t)}|BG)p(BG)}{p(\\overrightarrow{x}^{(t)}|FG)p(FG)}\n",
    "$$\n",
    "\n",
    "Where the aim is to determine the ratio between the probability of new pixel at time $t$ being a foreground ($FG$) or a background ($BG$).\n",
    "\n",
    "Since, in general, we don't have any prior information about $FG$, so we set it as a uniform distribution. Then, we decide that object is a $BG$ if the probability of $x$ at time $t$, given $BG$ is greater than some threshold value ($c_{thr}$):\n",
    "\n",
    "$$\n",
    "p(\\overrightarrow{x}^{(t)}|BG) > (=Rc_{FG})\n",
    "$$\n",
    "\n",
    "The left side of the equation is referred to as a background model. It depends on the training set denoted as $X$.\n",
    "\n",
    "In order to eliminate the problem of suddenly changing lighting factor, authors proposed to keep updating the training set by dropping old values, appending new ones and re-estimating the background model. When we incorporate the fact that within new frames we can have also foreground objects and if we use the Gaussian mixture model with M components, we can update the equation to the following one:\n",
    "\n",
    "$$\n",
    "\\hat{p}(\\overrightarrow{x}|X_T,BG+FG)=\\sum^{M}_{m=1}\\hat{\\pi}_m \\mathcal{N}(\\overrightarrow{x};\\hat{\\overrightarrow{\\mu}}_m,\\hat{\\sigma}^2_mI)\n",
    "$$\n",
    "\n",
    "Where we are adding means and variances, which describe the Gaussian components. Covariance matrices are\n",
    "diagonal and identity matrix has proper dimensions. The weights are non-negative and add up to 1.\n",
    "\n",
    "For each new data samples, equations are updated recursively, as follows:\n",
    "\n",
    "$$\n",
    "\\hat{\\pi}_m \\leftarrow \\hat{\\pi}_m + \\alpha(o^{(t)}_m-\\hat{\\pi}_m)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\overrightarrow{\\mu}}_m \\leftarrow \\hat{\\overrightarrow{\\mu}}_m + o^{(t)}_m (\\alpha/\\hat{\\pi}_m)\\overrightarrow{\\delta}_m\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2_m \\leftarrow \\hat{\\sigma}^2_m + o^{(t)}_m(\\alpha/\\hat{\\pi}_m)(\\overrightarrow{\\delta}^T_m \\overrightarrow{\\delta}_m  - \\hat{\\sigma}^2_m)\n",
    "$$\n",
    "\n",
    "We can see an introduction of alpha parameter here, which is exponentially decaying, meaning that the older\n",
    "data samples will be given less importance:\n",
    "\n",
    "$$\n",
    "\\alpha=1/T\n",
    "$$\n",
    "\n",
    "In the new sample the $o_m^{(t)}$ value is set to $1$ for the component with a largest weight and $0$ in other\n",
    "components.\n",
    "\n",
    "The following formula denotes the squared distance from m-th component:\n",
    "\n",
    "$$\n",
    "\\overrightarrow{\\delta}_m^T \\overrightarrow{\\delta}_m / \\hat{\\sigma}^2_m\n",
    "$$\n",
    "\n",
    "If the maximum number of components is reached, the component with the lowest weight is removed. Hence\n",
    "the algorithm has been defined by the author to be an \"online clustering algorithm\".\n",
    "\n",
    "Author also describes how model deals with foreground objects, which remain static for a longer duration of\n",
    "time: for the FG object to be considered a BG, it needs to be static for approx. following number of frames:\n",
    "\n",
    "$$\n",
    "log(1-c_f)/log(1-\\alpha)\n",
    "$$\n",
    "\n",
    "$c_f$ stands for the maximum portion of data, which belongs to $FG$ objects without influencing the $BG$ model.\n",
    "For sample values for $c_f$ and $\\alpha$, author has calculated that we would get $105$ frames for the $FG$ object to be\n",
    "considered a $BG$.\n",
    "\n",
    "Weights define the underlying multinomial distribution. After additional derivations, author rewrites the first\n",
    "equation to the following form (this is after including the *Dirichlet* prior for multinomial distribution):\n",
    "\n",
    "$$\n",
    "\\hat{\\pi}_m \\leftarrow \\hat{\\pi}_m + \\alpha(o^{(t)}_m-\\hat{\\pi}_m) - \\alpha c_T\n",
    "$$\n",
    "\n",
    "Then researcher presented a 3 great examples, where the new algorithm works well with improved performance.\n",
    "\n",
    "There are pre-requisites for this model to work:\n",
    "- stationary frame\n",
    "- image resized (in theory, this is not a strong requirement, but this model is very slow when used with the 1080p or even 720p image resolution)\n",
    "\n",
    "The OpenCV implementation for Python can be found under `cv2.createBackgroundSubtractorMOG2` function, which I have used to detect motion between consecutive image frames.\n",
    "\n",
    "The parameters will be explained in the later Chapter: [Data Collection](./04.DataCollection.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Object Detection\n",
    "\n",
    "Once motion is detected, resized frames can be sent to an Object Detector to analyse the content of an image.\n",
    "\n",
    "Object detection comes from two fundamental ideas in Computer Vision:\n",
    "- Image Classification (look at an image and classify a single class: Car, Person, Dog etc.)\n",
    "- Object Localization (where an object is located inside an image)\n",
    "\n",
    "Object Detection's task is to classify multiple objects in an image and tell their locations as well.\n",
    "\n",
    "Here is an example from a web application, which I have built for my research, which detects motion and object in a real time camera stream:\n",
    "\n",
    "<p style=\"text-align: center; margin-bottom: 0;\">Fig. 2.3. Real time detection in Web App</p>\n",
    "<img src=\"../Resources/img/person-detected.png\" style=\"width: 50%;\"/>\n",
    "\n",
    "In *Fig. 2.3* there is a person walking by in front of the parking area. There is a box around the person, called the *bounding box*. If there was more people in the frame, they would be all highlighted as well.\n",
    "\n",
    "This is an evolving field with new algorithms and applications been developed at high frequency. This is an advantage, as it creates new opportunities, but it's also a challenge for practicioners to keep up with the fast pace of change.\n",
    "\n",
    "Out of two arguably most popular options for object detection in Python: **Yolo** and **SSD**, I have decided to use Yolo (You Only Look Once) due to the fact that it can be run in real time on a GPU at 30+ frames per second with good documentation and wide-spread adoption rate. I have not found a GPU implementation for the SSD (Single Shot Detector) algorithm.\n",
    "\n",
    "#### Yolo V1:\n",
    "\n",
    "Even though in my research I have used Yolo Version 2, the section below is dedicated to Yolo version 1, as it is the foundation for object detection, which utilises multiple Computer Vision and Deep Learning paradims.\n",
    "\n",
    "Yolo v1 (Redmon et al., 2015) has been released in 2015 as a new approach to object detection, which promised extreme speed and making real time object detection a reality. This was a significant achievement in comparison to previous object detectors, like R-CNN (Girshick et al. 2013) where a single image could take 20 seconds to get processed or even in comparison to more modern Fast R-CNN (Girshick 2015), which still took 2 seconds to process a single image and Faster R-CNN (Ren et al., 2015) with 0.14 second per frame.\n",
    "\n",
    "The processing time can be extremely important for certain applications, like self driving car or camera monitoring system and, according to the authors, Yolo can run at 45 frames per second with a slight decrease in accuracy for smaller objects.\n",
    "\n",
    "To visualize this progress, I have put in into a table below:\n",
    "\n",
    "| Detector      | FPS |\n",
    "| ------------- | --- |\n",
    "| R-CNN | 0.05 |\n",
    "| Fast R-CNN  | 0.5 |\n",
    "| Faster R-CNN | 7 |\n",
    "| Yolo v1 | 45 |\n",
    "\n",
    "Yolo owes this gain in speed to a complete re-think of how the object detectors can operate. Instead of using a traditional approach, like:\n",
    "- sliding window with HOG (Histogram of Oriented Gradients) and SVM (Support Vertor Machine)\n",
    "- region proposals (seen in R-CNN's)\n",
    "\n",
    ", Yolo uses a single pass through the entire image to generate predictions (with sliding window it might be hundreds or thousands passes through an image of a different size),\n",
    "\n",
    "Then to get rid of overlapping bounding boxes, the boxes with very low propability are discarded, and Non-max supression algorithm is applied.\n",
    "\n",
    "This approach then leaves us with an output of 1470 feaures, containing all the data we need to understand the content of an image. Assuming that we are trying to predict any of 20 object classes, the calculation is as follows:\n",
    "\n",
    "$$\n",
    "7x7x(2x5+20) = 7x7x30tensor = 1470features\n",
    "$$\n",
    "\n",
    ", where $7x7$ is related to a grid size, which image is divided by, $2x5$ means we will have 2 bounding boxes inside each grid cell, and $20$ is a number of predicted classes in a One Hot Encoded notation.\n",
    "\n",
    "Each grid cell predicts two boxes and can only have a single class.\n",
    "\n",
    "Below is the name and description for each of the 5 nodes found in each bounding box:\n",
    "- Conf - confidence\n",
    "- x - x coordinate of center of the box (relative to grid cell)\n",
    "- y - y coordinate\n",
    "- w - width of the box (relative to whole image)\n",
    "- h - height\n",
    "\n",
    "The confidence if object is present is calculated as:\n",
    "$Pr(Object)*IOU^{truth}_{pred}$\n",
    "\n",
    ", where $IOU$ represents an Intersection Over Union, an evaluation metric for bounding boxes:\n",
    "\n",
    "$$\n",
    "IOU=\\frac{areaOfOverlap}{areaOfUnion}\n",
    "$$\n",
    "\n",
    "If there is no object present in the cell, then the confidence will be Zero, but if there is an object, it will equal to the $IOU$ metric.\n",
    "\n",
    "For training, a following Convolutional Neural network is used:\n",
    "\n",
    "<p style=\"text-align: center; margin-bottom: 0;\">Fig. 2.4. Yolo v1 architecture</p>\n",
    "<img src=\"../Resources/img/yolo-v1-architecture.png\" style=\"width: 75%;\"/>\n",
    "\n",
    ", with 24 convolutional layers followed by 2 fully connected layers. Convolutional layers are pretrained on ImageNet classification (with 1000 classes), and final output is, as expected, a $7x7x30$ tensor.\n",
    "\n",
    "This type of a complex network was trained for a week using the *Darknet* framework, which I have used for the real time inference in my research.\n",
    "\n",
    "The network is trained with $224x244$ image resolution and then extended to $444x448$ at detection stage.\n",
    "\n",
    "As activation function, authors have used a leaky rectified linear activation:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  f(x)=\\begin{cases}\n",
    "    x, & \\text{if $x>0$}\\\\\n",
    "    0.1x, & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The loss function to optimise is the highly customised $SumSquared Error$ with the specific characteristics:\n",
    "- to avoid issues with gradients for cells without any objects\n",
    "- to distinguish errors in large boxes versus small boxes (errors in large boxes matter less)\n",
    "\n",
    "Here are other significant training parameters:\n",
    "- epochs: $135$\n",
    "- batch size: $64$\n",
    "- momentum: $0.9$\n",
    "- decay: $0.0005$\n",
    "- custom learning rate schedule\n",
    "\n",
    "To prevent the overfit, dropout layers are introduced, and to increase image variability data augmentation is used.\n",
    "\n",
    "Overall, Yolo is a good tradeoff between speed and accuracy and at present is one of the most useful frameworks, which can be used free of cost with high level of available documentation and examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yolo V2:\n",
    "\n",
    "An upgraded version of Yolo (V2) was released in 2016 as a state of the art real time object detector capable of detecting over 9000 object categories.\n",
    "\n",
    "Capable of achieving a 76.8 mAP (mean average precision) on VOC 2007 (The PASCAL Visual Object Classes Challenge 2007) while maintaining 40 FPS (frames per second), which, as authors conclude, outperforms the other two most populate object detectors: *SSD* and *Faster RCNN with ResNet*.\n",
    "\n",
    "The main improvements in Yolo v2 have been achieved throgh a number of core ideas:\n",
    "- Batch Normalization: Added to all convolutional layers to stabilize training, speed up convergence and add regularisation\n",
    "- High Resolution Clasifier: End to end fully trained on $448x448$ image resolution, so more details can be detected\n",
    "- Convolutional With Anchor Boxes: Diving an image into N-overlapping boxes of WxH size - helpful to detect smaller objects, like multiple people faces\n",
    "- Dimension Clusters: Instead of hand picked anchor box dimensions, Yolo V2 uses k-means clustering with a custom distance metric $d(box,centroid)=1-IOU(box,centroid)$\n",
    "- Direct location prediction: Increase model stablity during early training iterations by introducing logistic activation to constrain network predictions or coordinates relative to the location of the cell grid\n",
    "- Multi-Scale Training: Aim is to make the model robust to varied image resolutions, which is achieved by randomly choosing a new image dimension every 10 batches during the training (size must be divisible by 32, from $320x320$ to $608x608$. When Yolo is run at $288x288$ it achieves much better performance, which might be useful for multiple video streams (for example one camera inside and two outside the house)\n",
    "\n",
    "The architecture is composed of 19 convolutional layers and 5 maxpolling layers. To process an image 5.58 billion operations is required. This might seem very high, but it is much lower in comparison to a very popular choice for feature extractor VGG-16 (Simonyan et al. 2014), which require 30.69 billion floating point operations. Yolo V1 required 8.52 billion, as it was based on the Googlenet architecture (Shegedy et al. 2014).\n",
    "\n",
    "Yolo V2 uses its own classification model called **Darknet-19**, which is trained for classification and for detection using slightly different architecture and hyper-parameters and similar data augmentation techniques.\n",
    "\n",
    "The *classification model* uses ImageNet dataset with 1000 very fine-grained classes (like \"Norfolk terrier\") and *detection model* uses COCO dataset with 80 high-level class names (like \"dog\").\n",
    "\n",
    "Since authors wanted to jointly train on classification and detection data, a **hierarchical classification** was used, where the final softmax layer with the flat encoding of mutually exclusive labels is not assumed. This has been achieved using an approach borrowed from Natural Language Processing called WordTree, where for each detail class a probability is calculated if this class belongs to a broader category, example:\n",
    "\n",
    "$$\n",
    "Pr(Norfolk terrier|terrier)\n",
    "$$\n",
    "\n",
    "This model has been visualized as follows:\n",
    "\n",
    "<p style=\"text-align: center; margin-bottom: 5px;\">Fig. 2.5. Yolo V2 WordTree</p>\n",
    "<img src=\"../Resources/img/word-tree.png\" style=\"width: 45%;\"/>\n",
    "\n",
    "#### Yolo V3, V4:\n",
    "\n",
    "Since 2016 and now, there have been already two new papers produced for the Yolo object detector:\n",
    "- version 3 (Redmon et al. 2018)\n",
    "- version 4 (Bochkovskiy et al. 2020)\n",
    "\n",
    "They both look very promising in terms of further accuracy increase with many tweaks and modern improvements, but I leave this work for the future increments of this project.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "Yolo V2 represents a good trade off between accuracy and performance, and has proven to work well in case of detecting people and vehicles from the Raspberry Pi camera frames, which I discuss further in the [Data Collection](./04.DataCollection.ipynb) chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Forecasting\n",
    "\n",
    "The next theme of this project is Forecasting. \n",
    "\n",
    "One of the research questions is if Machine Learning can improve the performance of the naive baseline, which predicts Object Counts based on the hourly averages calculated from the historical data.\n",
    "\n",
    "In my work I have utilised one naive and two Machine Learning algorithms, which are described below:\n",
    "- Gradient Boosting Decision Tree\n",
    "- Gaussian Process\n",
    "\n",
    "I have also tried many other algorithms, like:\n",
    "- Linear Regression\n",
    "- Feed Forward Neural Network\n",
    "- Long Short Term Memory Recurrent Neural Network\n",
    "\n",
    "The additional algorithms were not beneficial to my results due to the following reasons:\n",
    "- they are not designed to model non-linear relationships (Linear Regression)\n",
    "- they are more relevant for much larger datasets (Neural Networks overfitted very quickly and added computational overhead)\n",
    "- they either don't have a good implementation for the Count Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Gradient Boosting Decison Tree\n",
    "\n",
    "\n",
    "#### 2.2.1.1. Decision Tree Regressor\n",
    "\n",
    "**Decision Trees** are a building block for many more sophisticated Machine Learning algorithms. Their simplicity and interpretability make them a very popular choice, when decisions must be clearly understood and explained.\n",
    "\n",
    "The history of Decision Trees used for regression problems is not very easy to track, and goes back to a research by J.N. Morgan (Morgan et al., 1963, p. 430) titled *Problems in the Analysis of Survey Data, and a Proposal*, where arguably first decision tree for regression is drawn. What has been a challenge back then (computational overhead) is actually not an issue in 2020, which makes Decision Trees one of the fastest Machine Learners available for relatively condensed datasets.\n",
    "\n",
    "The problem with decision trees is their low accuracy on an out-of-sample datasets and low robustness (small changes in training data may easily lead to a very different tree). It is however important to discuss their inner workings before moving on to the more advanced algorithms.\n",
    "\n",
    "Here is an example output from a Tree Regressor algorithm run on a small subset of $100$ observations (for ilustrative purpose) from the dataset with people detections:\n",
    "\n",
    "<p style=\"text-align: center; margin-bottom: 5px;\">Fig. 2.6. Decision Tree</p>\n",
    "<img src=\"../Resources/img/tree.png\" style=\"width: 55%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tree can be interpreted as a series of sub-decisions to reach a goal. Following the **right hand side path**, the model predicts a value of $2.875$ by making the following decisions:\n",
    "- `uvIndex` is greater than $0.5$\n",
    "- `temperature` is greater than $2.99$\n",
    "- `hour` is greater than $10$\n",
    "\n",
    "It intuitively makes sense, as during the day, when temperature is not very low and after 10AM, we can expect approximately $3$ objects.\n",
    "\n",
    "Basic terminology related to the hierarchy above:\n",
    "- the single box on top of the diagram is called a **root node**\n",
    "- nodes in the middle are called the **decision nodes** and are connected by arrows creating a section called a **branch**\n",
    "- the eight boxes in the bottom are **leaf nodes**\n",
    "\n",
    "The best split for Regression Trees is usually calculated using a **mean squared error**, however other metrics (like a mean absolute error) can be utilised as well.\n",
    "\n",
    "The top-down procedure to generate a tree is the same for each node:\n",
    "- iterate through candidate features\n",
    "- for each feature:\n",
    "    - sort values\n",
    "    - find average between each pair of values and use as a candidate split value\n",
    "    - calculate average for values the left and right nodes\n",
    "    - calculate squared residuals for each node\n",
    "    - sum all residucals or average those\n",
    "- split the data by the feature and value, which produces the lowest squared error\n",
    "- keep doing this until:\n",
    "    - reached maximum depth of a tree allowed\n",
    "    - there is not enough samples to create a split\n",
    "    - all samples contain the same value\n",
    "- leaf nodes will eventually contain an average value for the target variable\n",
    "\n",
    "Hyperparameters `max_depth` and `min_samples_split` are used as a regularization term to avoid creating a model with too high variance.\n",
    "\n",
    "#### 2.2.1.2. Gradient Boosting Regressor Tree\n",
    "\n",
    "- aa\n",
    "- bb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Anomaly Detection\n",
    "\n",
    "The core idea of this system is anomaly detection. It makes it very useful as owners are often un-aware of the events in the observed area, and it is impractical to continuously monitor the enviornment.\n",
    "\n",
    "An anomaly detection solution described below, which uses **Probabilistic Programming** helps to identify a threshold of observations in a given hour, above which system can send an alert to the owners.\n",
    "\n",
    "But what if there is only a single unusual event in an hour? It turns out that we do not need to limit the system to a single anomaly detection algorithm. Below I will describe an **Auto-Encoder** - a Neural Network based method, which can detect an anomaly only from a single raw image frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Probabilistic Programming\n",
    "TODO: provide theoretical background here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Auto Encoders\n",
    "TODO: provide theoretical background here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Conclusion\n",
    "\n",
    "TODO: Here provide a high level conclusion and describe the next Chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[index](../Index.ipynb) | [prev](./01.Introduction.ipynb) | [next](./03.SystemDesign.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
