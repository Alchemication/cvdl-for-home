{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Notebook 1. Extract raw image data\n",
    "\n",
    "This Notebook is responsible for:\n",
    "- extracting objects and bounding boxes from the raw images\n",
    "- cleaning up detections registered with error\n",
    "- exporting a dataset with all detections for further analysis (into a parquet file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF -> Using GPU ->  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# import ConfigImports Notebook to import and configure libs\n",
    "%run ../Config/ConfigImports.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dates for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scan directory with images and find all folders corresponding to dates\n",
    "days_recorded = np.array(os.listdir(CONFIG['IMG_BASE_DIR']))\n",
    "days_recorded.sort()\n",
    "len(days_recorded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a DataFrame and remove dates outside of the scope for this research\n",
    "df = pd.DataFrame({'date': days_recorded.tolist()})\n",
    "df['date_dt'] = pd.to_datetime(df['date'])\n",
    "START_DATE, END_DATE = '2019-09-09', '2020-03-02'\n",
    "df = df.loc[(df['date_dt'] >= START_DATE) & (df['date_dt'] <= END_DATE)]\n",
    "\n",
    "# extract unique dates\n",
    "dates_found = df['date'].unique().tolist()\n",
    "n_dates = len(dates_found)\n",
    "n_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NN Yolo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing ./cfg/yolov2.cfg\n",
      "Parsing cfg/yolov2.cfg\n",
      "Loading weights/yolov2.weights ...\n",
      "Successfully identified 203934260 bytes\n",
      "Finished in 0.008756637573242188s\n",
      "\n",
      "Building net ...\n",
      "Source | Train? | Layer description                | Output size\n",
      "-------+--------+----------------------------------+---------------\n",
      "       |        | input                            | (?, 608, 608, 3)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 608, 608, 32)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 304, 304, 32)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 304, 304, 64)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 152, 152, 64)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 152, 152, 128)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 152, 152, 64)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 152, 152, 128)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 76, 76, 128)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 76, 76, 256)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 76, 76, 128)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 76, 76, 256)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 38, 38, 256)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 256)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 256)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 19, 19, 512)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 19, 19, 512)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 19, 19, 512)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | concat [16]                      | (?, 38, 38, 512)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 64)\n",
      " Load  |  Yep!  | local flatten 2x2                | (?, 19, 19, 256)\n",
      " Load  |  Yep!  | concat [27, 24]                  | (?, 19, 19, 1280)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 1x1p0_1    linear           | (?, 19, 19, 425)\n",
      "-------+--------+----------------------------------+---------------\n",
      "GPU mode with 0.25 usage\n",
      "Finished in 3.642862558364868s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "options = {\n",
    "    'model': 'cfg/yolov2.cfg',\n",
    "    'load': 'weights/yolov2.weights',\n",
    "    'threshold': 0.4,\n",
    "    'gpu': 0.5,\n",
    "    'verbalise': True\n",
    "}\n",
    "tfnet = TFNet(options);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process images\n",
    "\n",
    "This section is solving a very serious problem in the dataset.\n",
    "\n",
    "The problem occurs when the background subtraction algorithm detects a motion, but it's a False Positive (like wind causing tree branches to move). Normally such an image should be discared, but if there is a car parked outside the house - it will be picked up and there fore image saved as a valid observation.\n",
    "\n",
    "Code below detects these False Positives using a simple Computer Vision rule: predict labels for an image and remove any cars which are parked in front of the house. This simple approach works quite well, but as a future exercise, a more elegant solution to this problem should be identified.\n",
    "\n",
    "This step has been added to the real time data processing (in a *Custom Logic* step), and predictions from 26 November 2019 won't suffer from this problem any more.\n",
    "\n",
    "The code section below needs over five hours to complete, as it is essentially running Yolo detection again for all the collected images along with some custom logic.\n",
    "\n",
    "The goal is to create a set of csv files containing filenames of good images and bad images (which can be deleted as they have been registered erroneously)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_idx = 0\n",
    "# loop through all dates and print progress (using tqdm package)\n",
    "for i in tqdm(range(0, n_dates), ascii=True):\n",
    "    # initialize daily variables\n",
    "    dataset = []\n",
    "    dataset_rejected = []\n",
    "    d = dates_found[i]\n",
    "    # set up paths\n",
    "    images_path = CONFIG['IMG_BASE_DIR'] + '/' + d\n",
    "    images_found = os.listdir(images_path)\n",
    "    # iterate through images in a given day\n",
    "    for img in images_found:\n",
    "        objects_detected = img.split('_')[-1].replace('.jpg', '')\n",
    "        time_of_event = img.split('_')[0][:8]\n",
    "        im_path = images_path + '/' + img\n",
    "        img_orig = cv2.imread(im_path)\n",
    "        if img_orig is None:\n",
    "            print('Unable to open file {}'.format(im_path))\n",
    "            continue\n",
    "        img_sm = resize(img_orig.copy(), width=608, height=608)\n",
    "        # make predictions\n",
    "        results = tfnet.return_predict(img_sm)\n",
    "        # verify if valid objects were found\n",
    "        im_with_boxes, legit_boxes_info = draw_boxes(img_sm, results)\n",
    "        if len(legit_boxes_info) > 0:\n",
    "            for b in legit_boxes_info:\n",
    "                b.insert(0, im_idx)\n",
    "                b.append(d)\n",
    "                b.append(time_of_event)\n",
    "                b.append(img)\n",
    "                b.append(len(legit_boxes_info))\n",
    "                dataset.append(b)\n",
    "            im_idx += 1\n",
    "        else:\n",
    "            dataset_rejected.append([d, time_of_event, img])\n",
    "    # store daily csv file with good predictions\n",
    "    if len(dataset) > 0:\n",
    "        df_ok = pd.DataFrame(dataset, columns=['img_idx', 'label', 'confidence', 'x1', 'y1', 'x2', 'y2', \n",
    "                                           'date', 'time', 'filename', 'img_n_boxes']).to_csv(\n",
    "        './out/ok_{}.csv'.format(d), index=False)\n",
    "    # store daily csv file with FP predictions\n",
    "    if len(dataset_rejected) > 0:\n",
    "        df_rejected = pd.DataFrame(dataset_rejected, columns=['date', 'time', 'filename']).to_csv(\n",
    "            './out/rejected_{}.csv'.format(d), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete False Positive images from SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all csv files starting with rej*\n",
    "rejected_files = glob.glob('./out/rej*')\n",
    "rejected_files.sort()\n",
    "df_rej = pd.concat([pd.read_csv(f) for f in rejected_files])\n",
    "# remove images\n",
    "for index, row in df_rej.iterrows():\n",
    "    f_path = im_root + '/' + row['date'] + '/' + row['filename']\n",
    "    try:\n",
    "        os.unlink(f_path)\n",
    "    except:\n",
    "        pass  # forgive errors if file has been already deleted in the past"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather good images and create a DataFrame\n",
    "\n",
    "One row row below represents a single object detected in a frame, so there may be multiple records representing a single frame if more than one object was detected in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>img_idx</th>\n",
       "      <th>label</th>\n",
       "      <th>confidence</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>filename</th>\n",
       "      <th>img_n_boxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>72846</td>\n",
       "      <td>car</td>\n",
       "      <td>0.523175</td>\n",
       "      <td>298</td>\n",
       "      <td>7</td>\n",
       "      <td>426</td>\n",
       "      <td>71</td>\n",
       "      <td>2019-09-09</td>\n",
       "      <td>07.02.40</td>\n",
       "      <td>07.02.40.270_34c99836_car-car-car.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>72847</td>\n",
       "      <td>person</td>\n",
       "      <td>0.759682</td>\n",
       "      <td>489</td>\n",
       "      <td>31</td>\n",
       "      <td>518</td>\n",
       "      <td>106</td>\n",
       "      <td>2019-09-09</td>\n",
       "      <td>12.02.42</td>\n",
       "      <td>12.02.42.921_ea6c9143_person-bicycle.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>72847</td>\n",
       "      <td>bicycle</td>\n",
       "      <td>0.532076</td>\n",
       "      <td>444</td>\n",
       "      <td>54</td>\n",
       "      <td>484</td>\n",
       "      <td>100</td>\n",
       "      <td>2019-09-09</td>\n",
       "      <td>12.02.42</td>\n",
       "      <td>12.02.42.921_ea6c9143_person-bicycle.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>72848</td>\n",
       "      <td>person</td>\n",
       "      <td>0.864749</td>\n",
       "      <td>463</td>\n",
       "      <td>55</td>\n",
       "      <td>537</td>\n",
       "      <td>263</td>\n",
       "      <td>2019-09-09</td>\n",
       "      <td>07.30.02</td>\n",
       "      <td>07.30.02.409_c5662b14_person-car-car.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>72849</td>\n",
       "      <td>car</td>\n",
       "      <td>0.859297</td>\n",
       "      <td>302</td>\n",
       "      <td>23</td>\n",
       "      <td>410</td>\n",
       "      <td>73</td>\n",
       "      <td>2019-09-09</td>\n",
       "      <td>20.26.56</td>\n",
       "      <td>20.26.56.841_4ba2f42d_car.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  img_idx    label  confidence   x1  y1   x2   y2        date      time                                  filename  img_n_boxes\n",
       "0      0    72846      car    0.523175  298   7  426   71  2019-09-09  07.02.40     07.02.40.270_34c99836_car-car-car.jpg            1\n",
       "1      1    72847   person    0.759682  489  31  518  106  2019-09-09  12.02.42  12.02.42.921_ea6c9143_person-bicycle.jpg            2\n",
       "2      2    72847  bicycle    0.532076  444  54  484  100  2019-09-09  12.02.42  12.02.42.921_ea6c9143_person-bicycle.jpg            2\n",
       "3      3    72848   person    0.864749  463  55  537  263  2019-09-09  07.30.02  07.30.02.409_c5662b14_person-car-car.jpg            1\n",
       "4      4    72849      car    0.859297  302  23  410   73  2019-09-09  20.26.56             20.26.56.841_4ba2f42d_car.jpg            1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all csv files starting with ok*\n",
    "ok_files = glob.glob('./out/ok*')\n",
    "ok_files.sort()\n",
    "# put in a dataframe and show top 5 results\n",
    "df = pd.concat([pd.read_csv(f) for f in ok_files]).reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(657894, 12)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify how many results there is in total\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "person       333894\n",
       "car          238556\n",
       "truck         54166\n",
       "dog           14692\n",
       "bicycle        7241\n",
       "cat            5026\n",
       "bird           2351\n",
       "motorbike      1968\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify a count for each class type\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add date time related dimensions\n",
    "df['time_ms'] = df['filename'].str[9:12]\n",
    "df['date_time'] = pd.to_datetime((df['date'] + ' ' + df['time'].str.replace('.', ':') + '.' + df['time_ms'] + '00'), \n",
    "                                 format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "df['week_day'] = df['date_time'].dt.day_name()\n",
    "df['is_weekend'] = df['week_day'].isin(['Saturday', 'Sunday'])\n",
    "df['month'] = df['date_time'].dt.month\n",
    "df['hour'] = df['date_time'].dt.hour\n",
    "df['min'] = df['date_time'].dt.minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save into a Parquet file (efficient columnar format for large datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save into the efficient Parquet file\n",
    "df.to_parquet('res/results_2019-09-09_2020-03-02.parquet.gzip', engine='fastparquet', compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
